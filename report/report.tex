\documentclass[a4paper,oneside,article,11pt]{memoir}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}

\newtheorem*{toprove}{At bevise}
\newtheorem{lemma}{Lemma}

% This font looks so good.
\usepackage[sc]{mathpazo}

% Typesetting pseudo-code
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
% Code comments like [CLRS]
\renewcommand{\algorithmiccomment}[1]{\makebox[5cm][l]{$\triangleright$ \textit{#1}}}
\usepackage{framed,graphicx,xcolor}
\usepackage{listings}

\usepackage[font={small,it}]{caption}

% Relative references
\usepackage{varioref}

\usepackage{tikz}

\bibliographystyle{plain}

\title{Dynamic Algorithms \\ Dynamic Transitive Closure }
\author{Peter Gabrielsen 20114179 \\
Christoffer Hansen 20114637}
\newcounter{qcounter}
\begin{document}

\maketitle

\chapter{Introduction}
In this project we look at the dynamic transitive closure problem of a directed unweighted graph $G$.

\begin{itemize}
\item{\texttt{init($n$)}: $G$ becomes the empty directed graph with vertices $\left\lbrace 0,\dots, n-1\right\rbrace$ and no edges.}
\item{\texttt{insert($i,j$)}: Inserts an edge from $i$ to $j$ into $G$ (if it is not already contained in $G$).}
\item{\texttt{delete($i,j$)}: Deletes an edge from $i$ to $j$ from $G$ (if it is contained in $G$).}
\item{\texttt{transitiveClosure?}: Returns the total number of edges in the transitive closure graphs.}
\end{itemize}

We are going to implement two dynamic algorithms for this problem. The first, presented in section \ref{chap:fw}, is a trivial solution based on the Floyd-Warshall algorithm for computing all pairs shortest paths. The next solution, presented in \ref{chap:sankowski}, is based on Sankowskis randomized reduction to dynamic matrix adjoint combined with the use of the Sherman-Morrison formula.

The second solution requires us to do arithmetic modulo a prime $p$. We are thus going to describe algorithms for this in time $O(log^2 p)$ in section \ref{chap:prime}.

Finally we are going to experimentally test both algorithms and compare their running time on different inputs interesting to this problem.


\chapter{Floyd-Warshall}
\label{chap:fw}

Floyd-Warshalls algorithm is an algorithm for finding all-pairs shortest path in a directed graph. The algorithm in its traditional form uses a bottom-up dynamic programming approach to compute the shortest-path weights. It does so using the following recurrence:

$$
d_{ij}^{(k)} = \left\{ \begin{array}{ll}
w_{ij} &\mbox{ if $k=0$} \\
\min\left(d_{ij}^{(k-1)}, d_{ik}^{(k-1)} + d_{kj}^{(k-1)}\right) &\mbox{ if $k \geq 1$}
\end{array} \right.
$$

In the k'th iteration the Floyd-Warshall algorithm considers the shortest path from $i$ to $j$ with intermediate vertices in the set $\left\lbrace 1,2,\dots,k-1\right\rbrace$. It computes the k'th iteration by decomposing the path from $i$ to $j$ into $i \rightsquigarrow k \rightsquigarrow j$. Since $k$ is not an intermediate vertex of the path $i\rightsquigarrow k$ or $k\rightsquigarrow j$ we have that we have already computed the shortest path of these paths and can easily expand our solution using the recurrence.

We are, however, interested in computing the transitive closure of a directed graph and not all-pairs shortest path, but the Floyd-Warshall algorithm can be easily modified to solve this problem.

We do this by assigning a weight of 1 to each edge in our graph. If there's a path from $i$ to $j$ we will have that $d_{ij} < n$ otherwise $d_{ij} = \infty$. Another more efficient and simpler way is to replace the use of arithmetic operations with logical operations $\wedge$ and $\vee$. The k'th iteration computes whether there is a path from $i$ to $j$ by checking if it already exists or if it exists by considering the path $i\rightsquigarrow k\rightsquigarrow j$. By the same argument as before, we have already computed whether a path exists from $i$ to $k$ and $k$ to $j$. The recurrence becomes:

$$
t_{ij}^{(k)} = \left\{ \begin{array}{ll}
\left\{
\begin{array}{ll}
0 &\mbox{ if $i \neq j $ and $ (i,j)\not\in E$} \\
1 &\mbox{ if $i = j $ and $ (i,j)\in E$}
\end{array} 
\right.
&\mbox{ if $k=0$} \\
t_{ij}^{(k-1)} \vee \left(d_{ik}^{(k-1)} \wedge d_{kj}^{(k-1)}\right) &\mbox{ if $k \geq 1$}
\end{array} \right.
$$

The algorithm for computing the transitive closure using Floyd-Warshall is repeated in algorithm~\ref{alg:fw}. The complexity of this algorithm is trivially $\mathcal{O}(n^3)$ by the triple \texttt{for} loops.

The complexity of each operation is also quite simple. In the lazy solution we only compute the transitive closure when needed and we can thus do \texttt{insert} and \texttt{delete} in $\mathcal{O}(1)$. \texttt{init} can be done in $\mathcal{O}(1)$ using lazy initialization but our solution simply initializes the adjacency matrix and transitive closure graph upfront using $\mathcal{O}(n^2)$ operations. Finally the transitive closure can be computed in $\mathcal{O}(n^3)$.

Another solution is to eagerly update the transitive closure matrix on each \texttt{insert} and \texttt{delete} giving these operations a complexity of $\mathcal{O}(n^3)$. \texttt{init} has the same complexity as the lazy solution, but we are now able to report number of edges in the transitive closure graph in $\mathcal{O}(1)$.

The complexities are repeated in figure~\ref{fig:fw-comp}.

%algorithm
\begin{algorithm}[h]
\caption{\textsc{Floyd-Warshall Transitive Closure}(Graph $G$)}
\label{alg:fw}
\begin{algorithmic}[1]
\FOR{$k \leftarrow 1 $ to $n$}
	\FOR{$i \leftarrow 1 $ to $n$}
		\FOR{$j \leftarrow 1 $ to $n$}
			\STATE{$G_{ij} = G_{ij} \vee (G_{ik} \wedge G_{kj})$}
		\ENDFOR
	\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{figure}[h]
\center{
\begin{tabular}{|c|c|c|}
\hline
Operation & Lazy & Eager \\\hline
\texttt{init} & $\mathcal{O}(1)$ or $\mathcal{O}(n^2)$ & $\mathcal{O}(1)$ or $\mathcal{O}(n^2)$ \\
\texttt{insert} & $\mathcal{O}(1)$ & $\mathcal{O}(n^3)$ \\
\texttt{delete} & $\mathcal{O}(1)$ & $\mathcal{O}(n^3)$ \\
\texttt{transitiveClosure?} & $\mathcal{O}(n^3)$ & $\mathcal{O}(1)$ \\\hline
\end{tabular}
}
\caption{Complexities of each operation for the Floyd-Warshall solution.}
\label{fig:fw-comp}
\end{figure}

\chapter{Arithmetic modulo $p$}
\label{chap:prime}
In order to achieve the bounds presented in section~\ref{chap:sankowski} we need to be able to do fast arithmetic operations modulo $p$. We used that the operations can be performed in $\mathcal{O}(\log^2(p))$. This is trivially true for all operations except division, since they are simply the normal operation followed by a modulo operation.

We cannot just use normal division when in $\mathbb{Z}/p\mathbb{Z}$. We need to compute the modular inverse of the denominator and multiply this with the numerator modulo $p$. We need to show that the modular inverse can be computed in $\mathcal{O}(\log^2(p))$. We will compute the modular inverse using the extended euclidean algorithm. In addition to computing the greatest common divisor $d$ of $a$ and $b$, the extended euclidean algorithm also computes $x$ and $y$ such that $d = ax+by$ (BÃ©zout's identity). If we compute this identity with $b = p$ we have that $1 = ax+py$, since the greatest common divisor of $a$ and $p$ will always be $1$ in the field $\mathbb{Z}/p\mathbb{Z}$, i.e. all elements have an inverse. Now the modular inverse of $a$ is simply $x$ modulo $p$ since $py = 0$ in the field. Lets sum up these results in algorithm~\ref{alg:modinv} for computing the modular inverse modulo $p$.

%algorithm
\begin{algorithm}[H]
\caption{\textsc{Extended-Euclid}(a,b)}
\label{alg:extended_euclidean}
\begin{algorithmic}[1]
\IF{$b=0$}
	\RETURN{$(a,1,0)$}
\ELSE
	\STATE{$(d',x',y') = \textsc{Extended-Euclid}(b,\, a \mod b)$}
	\STATE{$(d,x,y) = (d',y',x'-\lfloor a/b \rfloor y')$}
	\RETURN{$(d,x,y)$}
\ENDIF
\end{algorithmic}
\end{algorithm}

%algorithm
\begin{algorithm}[H]
\caption{\textsc{Modulo Inverse}(a,b)}
\label{alg:modinv}
\begin{algorithmic}[1]
\STATE{$(d,x,y) = \textsc{Extended-Euclid}(a,b)$}
\IF{$d > 1$}
	\STATE{No inverse exists}
\ELSE
	\RETURN{$x \mod b$}
\ENDIF
\end{algorithmic}
\end{algorithm}

We see that the complexity is bounded by the complexity of computing the greatest common divisor in the extended euclidean algorithm.

\subsection{The running time of the Extended Euclidean algorithm}
The running time of this algorithm is equal to the number of recursive calls we make. The worst case analysis makes use of Fibonacci numbers, since calculating the greatest common divisor of two consecutive Fibonacci numbers will produce the worst case number of recursive calls. The analysis is similar to the one given in~\cite[p.~935-937]{clrs}.

The following lemma gives a condition on the size of the input in order to perform $k \geq 1$ recursive calls.

\begin{lemma}
\label{lemma:ee-rec}
If $a > b \geq 1$ and the call to \textsc{Extended-Euclid}(a,b) performs $k \geq 1$ recursive calls, then $a \geq F_{k+2}$ and $b \geq F_{k+1}$.
\end{lemma}
\begin{proof}
The proof is induction on $k$.

The basis of the induction is $k=1$. We must then have that $b \geq 1 = F_2$ and $a > b \Rightarrow a \geq 2 = F_3$. By the construction of Fibonacci numbers we have that $b > (a \mod b)$, which implies that, in each recursive call the first argument is strictly larger than the second.

Now assume that the lemma holds for $k-1$ recursive calls. We are now going to prove that it holds for $k$ recursive calls. We have that $b > 0$ and that \textsc{Extended-Euclid}(a,b) calls \textsc{Extended-Euclid}(b, a mod b) recursively, which by the induction hypothesis makes $k-1$ recursive calls. The induction hypothesis also tells us that $b \geq F_{k+1}$ and $a \mod b \geq F_k$.

We have that:
\begin{align*}
b+(a\mod b) &= b + (a-b\lfloor a/b \rfloor) \\
&\leq a
\end{align*}
since $a > b > 0 \Rightarrow \lfloor a/b \rfloor \geq 1$. It now follow that.
\begin{align*}
a &\geq b + (a \mod b) \\
&\geq F_{k+1} + F_k \\
&= F_{k+2}
\end{align*}
\end{proof}

LamÃ©'s theorem follows directly from lemma~\ref{lemma:ee-rec} and says that for any integer $k\geq 1$, if $a > b \geq 1$ and $b < F_{k+1}$, then the number of recursive calls is less than $k$.

It can be shown that the upper bound achieved from LamÃ©'s theorem is the best possible by looking at the call to \textsc{Extended-Euclid}($F_{k+1}$, $F_k$). By induction on $k$ we see that we will need $k-1$ recursive calls.
The inductive step is as follows:
\begin{align*}
\gcd(F_{k+1}, F_k) &= \gcd(F_k, F_{k+1} \mod F_k)\\
&= \gcd(F_k, F_{k-1})
\end{align*}

Since $F_k = \Theta(\phi^k)$ where $\phi$ is the golden ratio we have that the number of recursive calls is $\mathcal{O}(\lg_{\phi} b) = \mathcal{O}(\lg b)$. We perform a constant number of $\mathcal{O}(\lg p)$ arithmetic operations in each recursive call and we thus achieve a total complexity of $\mathcal{O}(\lg^2 p)$ for division modulo $p$.

\bibliography{references}
\end{document}


